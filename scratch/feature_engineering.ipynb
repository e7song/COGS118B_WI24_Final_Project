{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711e45da",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebdfc9",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66433489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kendrick/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kendrick/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "nltk.download(['vader_lexicon', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460e5bf",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/kkhandekar/word-difficulty/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa782de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Length</th>\n",
       "      <th>Freq_HAL</th>\n",
       "      <th>Log_Freq_HAL</th>\n",
       "      <th>I_Mean_RT</th>\n",
       "      <th>I_Zscore</th>\n",
       "      <th>I_SD</th>\n",
       "      <th>Obs</th>\n",
       "      <th>I_Mean_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>10610626</td>\n",
       "      <td>16.18</td>\n",
       "      <td>798.92</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>333.85</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aah</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>5.40</td>\n",
       "      <td>816.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>186.03</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron</td>\n",
       "      <td>5</td>\n",
       "      <td>10806</td>\n",
       "      <td>9.29</td>\n",
       "      <td>736.06</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>289.01</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aback</td>\n",
       "      <td>5</td>\n",
       "      <td>387</td>\n",
       "      <td>5.96</td>\n",
       "      <td>796.27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>171.61</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacus</td>\n",
       "      <td>6</td>\n",
       "      <td>513</td>\n",
       "      <td>6.24</td>\n",
       "      <td>964.40</td>\n",
       "      <td>0.65</td>\n",
       "      <td>489.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Length  Freq_HAL  Log_Freq_HAL  I_Mean_RT  I_Zscore    I_SD   Obs  \\\n",
       "0       a       1  10610626         16.18     798.92     -0.01  333.85  24.0   \n",
       "1     aah       3       222          5.40     816.43      0.21  186.03  21.0   \n",
       "2   Aaron       5     10806          9.29     736.06     -0.11  289.01  32.0   \n",
       "3   aback       5       387          5.96     796.27      0.11  171.61  15.0   \n",
       "4  abacus       6       513          6.24     964.40      0.65  489.00  15.0   \n",
       "\n",
       "   I_Mean_Accuracy  \n",
       "0             0.73  \n",
       "1             0.62  \n",
       "2             0.97  \n",
       "3             0.45  \n",
       "4             0.47  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/WordDifficulty.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33936890",
   "metadata": {},
   "source": [
    "### Feature Descriptions\n",
    "\n",
    "`Length`: Number of characters\n",
    "\n",
    "`Freq_HAL`: Hyperspace Analogue to Language frequency norms based on the HAL corpus of 131 million words. Higher values may indicate more frequent words in a corpus.\n",
    "\n",
    "`Log_Freq_HAL`: Applied logarithmic transformation to `Freq_HAL`\n",
    "\n",
    "`I_Mean_RT`: Individual mean reaction time, associated with lexical decision time\n",
    "\n",
    "`I_Zscore`: Z-score of individual reaction times, associated with word difficulty\n",
    "\n",
    "`I_SD`: Standard deviation of individual reaction times\n",
    "\n",
    "`Obs`: Number of observations/individuals experimented with respective word\n",
    "\n",
    "`I_Mean_Accuracy`: Individual mean accuracy score, average accuracy score in tasks related to word difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e4357",
   "metadata": {},
   "source": [
    "This dataset appears preprocessed prior at publishment. Performing our own transformations, such as normalization, may conflict with the prior. It is noted that `Freq_HAL` was computed respectively from the SUBTLEXUS corpus of 131 million words; therefore, it would be safe keep the data as is.\n",
    "\n",
    "From glance, we can perform a quick feature selection by removing the `Freq_HAL`, `I_Mean_RT`, `I_SD`, and `Obs` columns as other columns, such `Log_Freq_HAL`, `I_Mean_RT`, `I_Zscore` and mean scores, make them redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ba6e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Length</th>\n",
       "      <th>Log_Freq_HAL</th>\n",
       "      <th>I_Zscore</th>\n",
       "      <th>I_Mean_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>16.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aah</td>\n",
       "      <td>3</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaron</td>\n",
       "      <td>5</td>\n",
       "      <td>9.29</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aback</td>\n",
       "      <td>5</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacus</td>\n",
       "      <td>6</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Length  Log_Freq_HAL  I_Zscore  I_Mean_Accuracy\n",
       "0       a       1         16.18     -0.01             0.73\n",
       "1     aah       3          5.40      0.21             0.62\n",
       "2   aaron       5          9.29     -0.11             0.97\n",
       "3   aback       5          5.96      0.11             0.45\n",
       "4  abacus       6          6.24      0.65             0.47"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Freq_HAL and Obs columns\n",
    "df = df.drop(['Freq_HAL', 'I_Mean_RT', 'I_SD', 'Obs'], axis=1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Apply lower to words\n",
    "df['Word'] = df['Word'].str.lower()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256d77b",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "Use regex to remove quotes, astericks, and other punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e926c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[\\\"*!?.,']\"\n",
    "\n",
    "for index, word in enumerate(df['Word']):\n",
    "    cleaned_word = re.sub(pattern, '', word)\n",
    "    df.loc[index, 'Word'] = cleaned_word\n",
    "    \n",
    "# Remove duplicates\n",
    "df.drop_duplicates('Word', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c0adb",
   "metadata": {},
   "source": [
    "### Additional Quantitative Features \n",
    "\n",
    "We can extract additional quantitative features from a word, such as:\n",
    "\n",
    "- Vowel Count, also correlated to syllables\n",
    "- Entropy, or the measure of the unpredictability of the word's character. Computed from $H(x)=\\Sigma{p(x)\\log{p(x)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e39df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vowels\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "num_vowels = []\n",
    "for word in df['Word']:\n",
    "    vowel_count = sum(word.count(vowel) for vowel in vowels)\n",
    "    num_vowels.append(vowel_count)\n",
    "    \n",
    "df['Vowels'] = num_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d288eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word entropy\n",
    "def calculate_entropy(word):\n",
    "    # Frequency of each character\n",
    "    char_counts = Counter(word)\n",
    "\n",
    "    # Calculate the probability of each character\n",
    "    total_chars = len(word)\n",
    "    char_probabilities = {char: count / total_chars for char, count in char_counts.items()}\n",
    "\n",
    "    # Calculate the entropy\n",
    "    entropy = -sum(prob * math.log2(prob) for prob in char_probabilities.values())\n",
    "\n",
    "    return entropy\n",
    "\n",
    "entropy_values = [calculate_entropy(word) for word in df['Word']]\n",
    "df['Entropy'] = entropy_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5e56f",
   "metadata": {},
   "source": [
    "### Additional Features Using Natural Language Toolkit (NLTK) \n",
    "\n",
    "We will use `nltk` to get the parts-of-speech tag based on Penn [Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and a sentiment score for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bcb812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting parts of speech\n",
    "word_tags = nltk.pos_tag(df['Word'])\n",
    "\n",
    "word_tags = [word_tag[1] for word_tag in word_tags]\n",
    "\n",
    "df['PoS'] = word_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5d87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting sentiment score, we look at the compound score for a final vote\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = [sia.polarity_scores(word)['compound'] for word in df['Word']]\n",
    "\n",
    "df['SentimentScore'] = sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "229f873b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Length</th>\n",
       "      <th>Log_Freq_HAL</th>\n",
       "      <th>I_Zscore</th>\n",
       "      <th>I_Mean_Accuracy</th>\n",
       "      <th>Vowels</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>PoS</th>\n",
       "      <th>SentimentScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>16.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aah</td>\n",
       "      <td>3</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>0.918296</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaron</td>\n",
       "      <td>5</td>\n",
       "      <td>9.29</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3</td>\n",
       "      <td>1.921928</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aback</td>\n",
       "      <td>5</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>1.921928</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacus</td>\n",
       "      <td>6</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3</td>\n",
       "      <td>2.251629</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Length  Log_Freq_HAL  I_Zscore  I_Mean_Accuracy  Vowels   Entropy  \\\n",
       "0       a       1         16.18     -0.01             0.73       1 -0.000000   \n",
       "1     aah       3          5.40      0.21             0.62       2  0.918296   \n",
       "2   aaron       5          9.29     -0.11             0.97       3  1.921928   \n",
       "3   aback       5          5.96      0.11             0.45       2  1.921928   \n",
       "4  abacus       6          6.24      0.65             0.47       3  2.251629   \n",
       "\n",
       "  PoS  SentimentScore  \n",
       "0  DT             0.0  \n",
       "1  JJ             0.0  \n",
       "2  NN             0.0  \n",
       "3  NN             0.0  \n",
       "4  NN             0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('../data/NewWordDifficulty.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "965c547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein Distance... for algorithm distance metric later\n",
    "# word embeddings..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
